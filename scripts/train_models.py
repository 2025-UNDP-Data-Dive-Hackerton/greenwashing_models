#!/usr/bin/env python3
"""
ê·¸ë¦°ì›Œì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
UNDP Data Dive í•´ì»¤í†¤ 2025

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” 3ê°€ì§€ ML ëª¨ë¸ì„ í›ˆë ¨í•©ë‹ˆë‹¤:
1. í…ìŠ¤íŠ¸-íˆ¬ì ë¶ˆì¼ì¹˜ íƒì§€ (TF-IDF + Logistic Regression)
2. íˆ¬ì íŒ¨í„´ ì´ìƒ íƒì§€ (Isolation Forest)
3. ê¸°í›„ ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ íƒì§€ (Gradient Boosting)
"""

import duckdb
import pandas as pd
import numpy as np
import joblib
import json
import os
import re
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any

# ML ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import IsolationForest, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix

warnings.filterwarnings('ignore')

class GreenwashingModelTrainer:
    """ê·¸ë¦°ì›Œì‹± íƒì§€ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path: str = 'crs_processed.csv'):
        """
        ì´ˆê¸°í™”
        
        Args:
            data_path: ì „ì²˜ë¦¬ëœ CRS ë°ì´í„° ê²½ë¡œ
        """
        self.data_path = data_path
        self.models = {}
        self.config = {
            "environmental_keywords": [
                "climate", "green", "renewable", "environment", "environmental",
                "sustainable", "sustainability", "clean", "carbon", "emission",
                "solar", "wind", "energy efficiency", "biodiversity", "forest",
                "reforestation", "conservation", "ecosystem", "pollution"
            ],
            "anomaly_features": [
                "log_commitment", "log_disbursement", "climate_score",
                "marker_investment_ratio", "commitment_zscore", "climate_zscore",
                "year_normalized", "high_marker_low_investment"
            ],
            "inflation_features": [
                "total_projects", "mitigation_rate", "adaptation_rate",
                "environment_rate", "biodiversity_rate", "actual_green_investment_pct",
                "avg_investment", "high_marker_rate", "marker_reality_gap"
            ],
            "model_version": "1.0.0",
            "created_date": datetime.now().isoformat(),
            "performance_metrics": {}
        }
        
    def load_data(self) -> pd.DataFrame:
        """ë°ì´í„° ë¡œë”©"""
        print("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        if not os.path.exists(self.data_path):
            print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_path}")
            print("ğŸ’¡ ë¨¼ì € preprocess_crs_data.pyë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”!")
            raise FileNotFoundError(f"Data file not found: {self.data_path}")
        
        # DuckDBë¡œ ë°ì´í„° ë¡œë”©
        conn = duckdb.connect()
        df = conn.execute(f"""
            SELECT * FROM read_csv_auto('{self.data_path}', 
                                      ignore_errors=true, 
                                      null_padding=true)
        """).df()
        conn.close()
        
        print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(df):,}ê°œ ë ˆì½”ë“œ, {len(df.columns)}ê°œ ì»¬ëŸ¼")
        return df
    
    def create_text_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ íƒì§€ë¥¼ ìœ„í•œ í”¼ì²˜ ìƒì„±"""
        print("ğŸ”¤ í…ìŠ¤íŠ¸ í”¼ì²˜ ìƒì„± ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        df['ProjectTitle_clean'] = df['ProjectTitle'].fillna('').astype(str)
        df['ShortDescription_clean'] = df['ShortDescription'].fillna('').astype(str)
        df['combined_text'] = df['ProjectTitle_clean'] + ' ' + df['ShortDescription_clean']
        
        # í™˜ê²½ í‚¤ì›Œë“œ ì ìˆ˜ ê³„ì‚°
        def calculate_env_score(text):
            if pd.isna(text) or text == '':
                return 0
            text_lower = str(text).lower()
            return sum(1 for keyword in self.config["environmental_keywords"] 
                      if keyword in text_lower)
        
        df['env_keyword_count'] = df['combined_text'].apply(calculate_env_score)
        
        # ê¸°í›„ ë§ˆì»¤ ì´í•©
        climate_cols = ['ClimateMitigation', 'ClimateAdaptation', 'Environment', 'Biodiversity']
        for col in climate_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        df['total_climate_markers'] = df[climate_cols].sum(axis=1)
        
        # ë¶ˆì¼ì¹˜ ë ˆì´ë¸” ìƒì„± (í™˜ê²½ í‚¤ì›Œë“œ ë§ì§€ë§Œ ê¸°í›„ ë§ˆì»¤ ì—†ìŒ)
        text_inconsistency = ((df['env_keyword_count'] >= 2) & 
                             (df['total_climate_markers'] == 0)).astype(int)
        
        # TF-IDF ë²¡í„°í™”
        tfidf = TfidfVectorizer(
            max_features=5000,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )
        
        X_text = tfidf.fit_transform(df['combined_text'])
        self.models['tfidf_vectorizer'] = tfidf
        
        print(f"âœ… í…ìŠ¤íŠ¸ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {X_text.shape}")
        print(f"   - ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤: {text_inconsistency.sum():,}ê°œ ({text_inconsistency.mean():.1%})")
        
        return X_text, text_inconsistency
    
    def create_anomaly_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """íˆ¬ì ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ í”¼ì²˜ ìƒì„±"""
        print("ğŸ“ˆ íˆ¬ì ì´ìƒ í”¼ì²˜ ìƒì„± ì¤‘...")
        
        # ìˆ˜ì¹˜í˜• ë³€í™˜
        df['USD_Commitment'] = pd.to_numeric(df['USD_Commitment'], errors='coerce').fillna(0)
        df['USD_Disbursement'] = pd.to_numeric(df['USD_Disbursement'], errors='coerce').fillna(0)
        
        # ë¡œê·¸ ë³€í™˜ (0ê°’ ì²˜ë¦¬)
        df['log_commitment'] = np.log1p(df['USD_Commitment'])
        df['log_disbursement'] = np.log1p(df['USD_Disbursement'])
        
        # ê¸°í›„ ì ìˆ˜
        df['climate_score'] = df['total_climate_markers'] / 4.0  # 0-1 ì •ê·œí™”
        
        # ë§ˆì»¤-íˆ¬ì ë¹„ìœ¨
        df['marker_investment_ratio'] = np.where(
            df['USD_Commitment'] > 0,
            df['total_climate_markers'] / (df['USD_Commitment'] / 1000000),  # ë°±ë§Œë‹¬ëŸ¬ë‹¹
            0
        )
        
        # Z-score ì •ê·œí™”
        df['commitment_zscore'] = (df['log_commitment'] - df['log_commitment'].mean()) / df['log_commitment'].std()
        df['climate_zscore'] = (df['climate_score'] - df['climate_score'].mean()) / df['climate_score'].std()
        
        # ì—°ë„ ì •ê·œí™” (Year ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©)
        df['year_normalized'] = 0.5  # ê¸°ë³¸ê°’ (2018-2019ë…„ ì •ë„ë¡œ ê°€ì •)
        
        # ë†’ì€ ë§ˆì»¤ + ë‚®ì€ íˆ¬ì í”Œë˜ê·¸
        df['high_marker_low_investment'] = (
            (df['total_climate_markers'] >= 2) & 
            (df['USD_Commitment'] < df['USD_Commitment'].quantile(0.25))
        ).astype(int)
        
        # í”¼ì²˜ ì„ íƒ
        feature_cols = [col for col in self.config["anomaly_features"] if col in df.columns]
        X_anomaly = df[feature_cols].fillna(0).values
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        X_anomaly_scaled = scaler.fit_transform(X_anomaly)
        self.models['anomaly_scaler'] = scaler
        
        # ì´ìƒì¹˜ ë ˆì´ë¸” (ì‹¤ì œë¡œëŠ” ë¹„ì§€ë„ í•™ìŠµì´ì§€ë§Œ í‰ê°€ìš©)
        anomaly_labels = np.zeros(len(df))  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
        
        print(f"âœ… íˆ¬ì ì´ìƒ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {X_anomaly_scaled.shape}")
        
        return X_anomaly_scaled, anomaly_labels
    
    def create_inflation_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ íƒì§€ë¥¼ ìœ„í•œ í”¼ì²˜ ìƒì„±"""
        print("ğŸ¯ ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ í”¼ì²˜ ìƒì„± ì¤‘...")
        
        # êµ­ê°€ë³„ ì§‘ê³„
        country_stats = df.groupby('DonorName').agg({
            'ProjectTitle': 'count',
            'ClimateMitigation': ['mean', 'sum'],
            'ClimateAdaptation': ['mean', 'sum'],
            'Environment': ['mean', 'sum'],
            'Biodiversity': ['mean', 'sum'],
            'USD_Commitment': ['mean', 'sum'],
            'total_climate_markers': 'mean'
        }).round(4)
        
        # ì»¬ëŸ¼ëª… ì •ë¦¬
        country_stats.columns = [
            'total_projects', 'mitigation_rate', 'mitigation_sum',
            'adaptation_rate', 'adaptation_sum', 'environment_rate', 'environment_sum',
            'biodiversity_rate', 'biodiversity_sum', 'avg_investment', 'total_investment',
            'avg_climate_score'
        ]
        
        # ì¶”ê°€ í”¼ì²˜ ê³„ì‚°
        country_stats['actual_green_investment_pct'] = (
            country_stats['total_investment'] * country_stats['avg_climate_score'] / 
            country_stats['total_investment'].clip(lower=1)
        ) * 100
        
        country_stats['high_marker_rate'] = (
            (country_stats['mitigation_rate'] + country_stats['adaptation_rate'] + 
             country_stats['environment_rate'] + country_stats['biodiversity_rate']) >= 2
        ).astype(float)
        
        country_stats['marker_reality_gap'] = (
            country_stats['avg_climate_score'] - 
            country_stats['actual_green_investment_pct'] / 100
        )
        
        # í”¼ì²˜ ì„ íƒ
        feature_cols = [col for col in self.config["inflation_features"] 
                       if col in country_stats.columns]
        X_inflation = country_stats[feature_cols].fillna(0).values
        
        # ì¸í”Œë ˆì´ì…˜ ë ˆì´ë¸” ìƒì„± (ë†’ì€ ë§ˆì»¤ + ë‚®ì€ ì‹¤ì œ íˆ¬ì)
        inflation_labels = (
            (country_stats['high_marker_rate'] > 0.3) & 
            (country_stats['actual_green_investment_pct'] < 20)
        ).astype(int)
        
        print(f"âœ… ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {X_inflation.shape}")
        print(f"   - ì¸í”Œë ˆì´ì…˜ ì˜ì‹¬ êµ­ê°€: {inflation_labels.sum()}ê°œ ({inflation_labels.mean():.1%})")
        
        return X_inflation, inflation_labels
    
    def train_text_model(self, X_text: np.ndarray, y_text: np.ndarray) -> float:
        """í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ”¤ í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_text, y_text, test_size=0.2, random_state=42, stratify=y_text
        )
        
        # ëª¨ë¸ í›ˆë ¨
        model = LogisticRegression(
            random_state=42,
            max_iter=1000,
            class_weight='balanced'
        )
        model.fit(X_train, y_train)
        
        # í‰ê°€
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        print(f"âœ… í…ìŠ¤íŠ¸ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ - AUC: {auc_score:.4f}")
        
        # ëª¨ë¸ ì €ì¥
        self.models['text_inconsistency_model'] = model
        
        return auc_score
    
    def train_anomaly_model(self, X_anomaly: np.ndarray, y_anomaly: np.ndarray) -> float:
        """íˆ¬ì ì´ìƒ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ“ˆ íˆ¬ì ì´ìƒ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        # Isolation Forest í›ˆë ¨
        model = IsolationForest(
            contamination=0.05,  # 5% ì´ìƒì¹˜ ê°€ì •
            random_state=42,
            n_estimators=100
        )
        model.fit(X_anomaly)
        
        # ì´ìƒì¹˜ íƒì§€
        anomaly_scores = model.decision_function(X_anomaly)
        anomaly_predictions = model.predict(X_anomaly)
        
        # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚°
        anomaly_rate = (anomaly_predictions == -1).mean()
        
        print(f"âœ… íˆ¬ì ì´ìƒ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ - ì´ìƒì¹˜ ë¹„ìœ¨: {anomaly_rate:.1%}")
        
        # ëª¨ë¸ ì €ì¥
        self.models['investment_anomaly_model'] = model
        
        return anomaly_rate
    
    def train_inflation_model(self, X_inflation: np.ndarray, y_inflation: np.ndarray) -> float:
        """ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸ¯ ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        
        if len(np.unique(y_inflation)) < 2:
            print("âš ï¸  ì¸í”Œë ˆì´ì…˜ ë ˆì´ë¸”ì´ ë‹¨ì¼ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì„ì˜ ë ˆì´ë¸” ìƒì„±...")
            # ìƒìœ„ 20% êµ­ê°€ë¥¼ ì¸í”Œë ˆì´ì…˜ìœ¼ë¡œ ì„¤ì •
            threshold = np.percentile(X_inflation[:, -1], 80)  # marker_reality_gap ê¸°ì¤€
            y_inflation = (X_inflation[:, -1] > threshold).astype(int)
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_inflation, y_inflation, test_size=0.2, random_state=42, stratify=y_inflation
        )
        
        # ëª¨ë¸ í›ˆë ¨
        model = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            random_state=42
        )
        model.fit(X_train, y_train)
        
        # í‰ê°€
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        auc_score = roc_auc_score(y_test, y_pred_proba)
        
        print(f"âœ… ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ - AUC: {auc_score:.4f}")
        
        # ëª¨ë¸ ì €ì¥
        self.models['marker_inflation_model'] = model
        
        return auc_score
    
    def save_models(self):
        """ëª¨ë¸ ë° ì„¤ì • ì €ì¥"""
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        
        models_dir = 'models'
        os.makedirs(models_dir, exist_ok=True)
        
        # ê°œë³„ ëª¨ë¸ ì €ì¥
        for model_name, model in self.models.items():
            model_path = os.path.join(models_dir, f'{model_name}.pkl')
            joblib.dump(model, model_path)
            print(f"   âœ… {model_name}.pkl ì €ì¥ ì™„ë£Œ")
        
        # ì„¤ì • ì €ì¥
        config_path = os.path.join(models_dir, 'model_config.json')
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, indent=2, ensure_ascii=False)
        print(f"   âœ… model_config.json ì €ì¥ ì™„ë£Œ")
        
        print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
    
    def train_all_models(self):
        """ì „ì²´ ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸"""
        print("ğŸš€ ê·¸ë¦°ì›Œì‹± íƒì§€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        print("=" * 50)
        
        # 1. ë°ì´í„° ë¡œë”©
        df = self.load_data()
        
        # 2. í…ìŠ¤íŠ¸ ëª¨ë¸ í›ˆë ¨
        X_text, y_text = self.create_text_features(df)
        text_auc = self.train_text_model(X_text, y_text)
        self.config['performance_metrics']['text_model_auc'] = text_auc
        
        print("-" * 50)
        
        # 3. íˆ¬ì ì´ìƒ ëª¨ë¸ í›ˆë ¨
        X_anomaly, y_anomaly = self.create_anomaly_features(df)
        anomaly_rate = self.train_anomaly_model(X_anomaly, y_anomaly)
        self.config['performance_metrics']['anomaly_detection_rate'] = anomaly_rate
        
        print("-" * 50)
        
        # 4. ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ ëª¨ë¸ í›ˆë ¨
        X_inflation, y_inflation = self.create_inflation_features(df)
        inflation_auc = self.train_inflation_model(X_inflation, y_inflation)
        self.config['performance_metrics']['inflation_model_auc'] = inflation_auc
        
        print("-" * 50)
        
        # 5. ëª¨ë¸ ì €ì¥
        self.save_models()
        
        print("\nğŸ‰ í›ˆë ¨ ì™„ë£Œ! ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ AUC: {text_auc:.4f}")
        print(f"   ğŸ“Š íˆ¬ì ì´ìƒ íƒì§€ìœ¨: {anomaly_rate:.1%}")
        print(f"   ğŸ¯ ë§ˆì»¤ ì¸í”Œë ˆì´ì…˜ AUC: {inflation_auc:.4f}")
        print("\nğŸ’¡ ì´ì œ predict_greenwashing.pyë¡œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¤– UNDP ê·¸ë¦°ì›Œì‹± íƒì§€ ëª¨ë¸ í›ˆë ¨ê¸°")
    print("=" * 50)
    
    # ë°ì´í„° íŒŒì¼ í™•ì¸
    data_path = 'crs_processed.csv'
    if not os.path.exists(data_path):
        print("âŒ ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
        print("   python scripts/preprocess_crs_data.py")
        return
    
    try:
        # í›ˆë ¨ ì‹œì‘
        trainer = GreenwashingModelTrainer(data_path)
        trainer.train_all_models()
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ğŸ’¡ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ë³´ì„¸ìš”.")
        raise

if __name__ == "__main__":
    main() 