# 5. **`batch_analyze(projects_df)`** - ëŒ€ëŸ‰ í”„ë¡œì íŠ¸ ì¼ê´„ ë¶„ì„ (ML ëª¨ë¸ í•„ìˆ˜)

import pandas as pd
import re
import joblib
import os

class IndependentGreenwashingDetector:
    """
    ML ê¸°ë°˜ ê·¸ë¦°ì›Œì‹± íƒì§€ê¸° (TF-IDF + LogisticRegression í•„ìˆ˜)
    """
    
    def __init__(self, 
                 model_path='scripts/tuning/models/text_inconsistency/best_model_20250731_191849.pkl',
                 tfidf_path='scripts/tuning/models/individual/tfidf_vectorizer.pkl'):
        # í™˜ê²½ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’)
        self.environmental_keywords = [
            "climate", "green", "renewable", "environment", "environmental",
            "sustainable", "sustainability", "clean", "carbon", "emission",
            "solar", "wind", "energy efficiency", "biodiversity", "forest",
            "reforestation", "conservation", "ecosystem", "pollution"
        ]
        
        # ML ëª¨ë¸ ë¡œë“œ (í•„ìˆ˜)
        self.ml_model = None
        self.tfidf_vectorizer = None
        
        # 1. ML ëª¨ë¸ ë¡œë“œ
        self.ml_model = self._load_model_file(model_path, "ML ëª¨ë¸")
        
        # 2. TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ (ëª¨ë¸ íŒŒì¼ì—ì„œ ë¨¼ì € ì‹œë„)
        self.tfidf_vectorizer = self._load_tfidf_vectorizer(tfidf_path)
        
        print("âœ… ëª¨ë“  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ!")
        print(f"ëª¨ë¸ íƒ€ì…: {type(self.ml_model).__name__}")
        print(f"ë²¡í„°ë¼ì´ì € íƒ€ì…: {type(self.tfidf_vectorizer).__name__}")
        print(f"í™˜ê²½ í‚¤ì›Œë“œ ê°œìˆ˜: {len(self.environmental_keywords)}ê°œ")
        
        # íŠ¹ì„± ìˆ˜ í˜¸í™˜ì„± í™•ì¸
        if hasattr(self.ml_model, 'n_features_'):
            print(f"ëª¨ë¸ ì˜ˆìƒ íŠ¹ì„± ìˆ˜: {self.ml_model.n_features_}")
        if hasattr(self.tfidf_vectorizer, 'vocabulary_'):
            print(f"ë²¡í„°ë¼ì´ì € íŠ¹ì„± ìˆ˜: {len(self.tfidf_vectorizer.vocabulary_)}")
            
        # ì „ì²´ íŠ¹ì„± ìˆ˜ ê³„ì‚° ë° í˜¸í™˜ì„± ì²´í¬
        if hasattr(self, 'preprocessors') and self.preprocessors:
            total_features = 0
            for key, processor in self.preprocessors.items():
                if hasattr(processor, 'vocabulary_'):
                    total_features += len(processor.vocabulary_)
                elif hasattr(processor, 'classes_'):
                    total_features += 1  # ë ˆì´ë¸” ì¸ì½”ë”ëŠ” 1ê°œ íŠ¹ì„±
            
            print(f"ê³„ì‚°ëœ ì „ì²´ íŠ¹ì„± ìˆ˜: {total_features}")
            
            if hasattr(self.ml_model, 'n_features_') and total_features != self.ml_model.n_features_:
                raise ValueError(f"ML ëª¨ë¸ê³¼ ì „ì²´ íŠ¹ì„± ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
                               f"ëª¨ë¸: {self.ml_model.n_features_}, ê³„ì‚°ëœ íŠ¹ì„±: {total_features}")
        
        print("âœ… ML ëª¨ë¸ê³¼ ì „ì²´ íŠ¹ì„± í˜¸í™˜ì„± í™•ì¸ ì™„ë£Œ!")
    
    def _load_model_file(self, file_path, description):
        """ëª¨ë¸ íŒŒì¼ ë¡œë“œ í—¬í¼ í•¨ìˆ˜"""
        possible_paths = [
            file_path,
            f"../{file_path}",
            os.path.join(os.path.dirname(__file__), f"../{file_path}"),
            os.path.join(os.getcwd(), file_path),
            os.path.abspath(file_path),
            os.path.abspath(f"../{file_path}")
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                try:
                    print(f"ğŸ“š {description} ë¡œë“œ ì¤‘: {path}")
                    print(f"íŒŒì¼ í¬ê¸°: {os.path.getsize(path) / (1024*1024):.1f} MB")
                    
                    # íŒŒì¼ ë¡œë“œ
                    data = joblib.load(path)
                    
                    # ëª¨ë¸ íŒŒì¼ì¸ ê²½ìš° êµ¬ì¡° ë¶„ì„ ë° í™˜ê²½ í‚¤ì›Œë“œ ì¶”ì¶œ
                    if description == "ML ëª¨ë¸" and isinstance(data, dict):
                        available_keys = list(data.keys())
                        print(f"ğŸ“‹ ëª¨ë¸ íŒŒì¼ êµ¬ì¡°: {available_keys}")
                        
                        # Preprocessors êµ¬ì¡° í™•ì¸
                        preprocessors = data.get('preprocessors', {})
                        if isinstance(preprocessors, dict):
                            print(f"ğŸ“‹ Preprocessors êµ¬ì¡°: {list(preprocessors.keys())}")
                        
                        # í™˜ê²½ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
                        if 'env_keywords' in data:
                            file_keywords = data['env_keywords']
                            if isinstance(file_keywords, list) and len(file_keywords) > 0:
                                self.environmental_keywords = file_keywords
                                print(f"ğŸ“‹ ëª¨ë¸ íŒŒì¼ì—ì„œ {len(file_keywords)}ê°œ í™˜ê²½ í‚¤ì›Œë“œ ë¡œë“œ")
                        
                        # ëª¨ë¸ ì¶”ì¶œ
                        model = data.get('model')
                        if model is None:
                            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤: {available_keys}")
                        
                        # ëª¨ë¸ê³¼ í•¨ê»˜ ì „ì²´ ë°ì´í„°ë„ ì €ì¥ (ë‚˜ì¤‘ì— preprocessors ì ‘ê·¼ìš©)
                        self._model_data = data
                        return model
                    else:
                        # TF-IDF ë²¡í„°ë¼ì´ì €ëŠ” ì§ì ‘ ë°˜í™˜
                        return data
                        
                except Exception as e:
                    print(f"âš ï¸  {path}ì—ì„œ {description} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        
        # ëª¨ë“  ê²½ë¡œì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
        print(f"âŒ {description} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        print(f"ì‹œë„í•œ ê²½ë¡œë“¤:")
        for path in possible_paths:
            print(f"  - {path} (ì¡´ì¬: {os.path.exists(path)})")
        raise FileNotFoundError(f"{description}ì´ í•„ìˆ˜ì…ë‹ˆë‹¤. íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _load_tfidf_vectorizer(self, tfidf_path):
        """TF-IDF ë²¡í„°ë¼ì´ì € ë¡œë“œ ë° íŠ¹ì„± ì¡°í•©"""
        
        # 1. ëª¨ë¸ íŒŒì¼ì˜ preprocessorsì—ì„œ ëª¨ë“  íŠ¹ì„± ì¶”ì¶œ
        if hasattr(self, '_model_data') and self._model_data:
            preprocessors = self._model_data.get('preprocessors', {})
            if isinstance(preprocessors, dict):
                print("ğŸ“š ëª¨ë¸ íŒŒì¼ì˜ preprocessors êµ¬ì¡° ë¶„ì„ ì¤‘...")
                
                # ê° preprocessorì˜ íŠ¹ì„± ìˆ˜ í™•ì¸
                for key, processor in preprocessors.items():
                    if hasattr(processor, 'vocabulary_'):
                        print(f"  - {key}: {len(processor.vocabulary_)}ê°œ íŠ¹ì„±")
                    elif hasattr(processor, 'classes_'):
                        print(f"  - {key}: {len(processor.classes_)}ê°œ í´ë˜ìŠ¤")
                
                # tfidf_titleì„ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ì „ì²´ íŠ¹ì„± êµ¬ì¡° ì €ì¥
                self.preprocessors = preprocessors
                
                # ì£¼ TF-IDF ë²¡í„°ë¼ì´ì € ì„ íƒ (tfidf_title ìš°ì„ )
                main_tfidf = (preprocessors.get('tfidf_title') or 
                             preprocessors.get('tfidf_vectorizer') or 
                             preprocessors.get('tfidf'))
                
                if main_tfidf is not None:
                    print(f"ğŸ“š ë©”ì¸ TF-IDF ë²¡í„°ë¼ì´ì €: tfidf_title")
                    print(f"ë²¡í„°ë¼ì´ì € íƒ€ì…: {type(main_tfidf).__name__}")
                    if hasattr(main_tfidf, 'vocabulary_'):
                        print(f"ë©”ì¸ ë²¡í„°ë¼ì´ì € íŠ¹ì„± ìˆ˜: {len(main_tfidf.vocabulary_)}")
                    return main_tfidf
        
        # 2. ë³„ë„ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
        print("ğŸ“š ëª¨ë¸ íŒŒì¼ì—ì„œ TF-IDFë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë³„ë„ íŒŒì¼ì—ì„œ ì‹œë„")
        return self._load_model_file(tfidf_path, "TF-IDF ë²¡í„°ë¼ì´ì €")
    
    def _create_full_features(self, title_clean):
        """ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì „ì²´ íŠ¹ì„± ë²¡í„° ìƒì„± (3002ê°œ)"""
        import numpy as np
        from scipy.sparse import hstack
        
        # 1. tfidf_title (1000ê°œ íŠ¹ì„±)
        title_features = self.preprocessors['tfidf_title'].transform([title_clean])
        
        # 2. tfidf_desc (2000ê°œ íŠ¹ì„±) - ë¹ˆ ì„¤ëª…ìœ¼ë¡œ ì²˜ë¦¬
        desc_features = self.preprocessors['tfidf_desc'].transform([''])
        
        # 3. le_sector (1ê°œ íŠ¹ì„±) - ê¸°ë³¸ê°’ 0
        sector_features = np.array([[0]])
        
        # 4. le_purpose (1ê°œ íŠ¹ì„±) - ê¸°ë³¸ê°’ 0  
        purpose_features = np.array([[0]])
        
        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        combined_features = hstack([title_features, desc_features, sector_features, purpose_features])
        
        return combined_features
    
    def preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        if not text:
            return ""
        
        text = str(text).lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def detect_greenwashing(self, project_data):
        """
        ML ê¸°ë°˜ ê·¸ë¦°ì›Œì‹± íƒì§€ ë¡œì§
        - í‚¤ì›Œë“œì™€ ë§ˆì»¤ì˜ ë¶ˆì¼ì¹˜ íƒì§€
        - ML ëª¨ë¸ì„ í†µí•œ ì •í™•í•œ ì˜ˆì¸¡
        """
        title = project_data.get('ProjectTitle', '')
        climate_mitigation = project_data.get('ClimateMitigation', 0)
        climate_adaptation = project_data.get('ClimateAdaptation', 0)
        environment = project_data.get('Environment', 0)
        biodiversity = project_data.get('Biodiversity', 0)
        
        total_markers = climate_mitigation + climate_adaptation + environment + biodiversity
        
        # í™˜ê²½ í‚¤ì›Œë“œ íƒì§€
        title_clean = self.preprocess_text(title)
        detected_keywords = []
        for keyword in self.environmental_keywords:
            if keyword.lower() in title_clean:
                detected_keywords.append(keyword)
        
        # ML ëª¨ë¸ ì˜ˆì¸¡ (ìˆœìˆ˜ MLë§Œ ì‚¬ìš©)
        ml_probability = 0.0
        if detected_keywords:  # í™˜ê²½ í‚¤ì›Œë“œê°€ ìˆì„ ë•Œë§Œ ML ì˜ˆì¸¡
            # ëª¨ë“  íŠ¹ì„±ì„ ê²°í•©í•˜ì—¬ ì˜ˆì¸¡ (3002ê°œ íŠ¹ì„± ë§ì¶”ê¸°)
            features = self._create_full_features(title_clean)
            # ML ëª¨ë¸ë¡œ ì˜ˆì¸¡
            ml_probability = float(self.ml_model.predict_proba(features)[0][1])
        
        # ê·¸ë¦°ì›Œì‹± íŒì • ë¡œì§ ê°œì„  (ê·œì¹™ ê¸°ë°˜ + ML)
        has_keywords = len(detected_keywords) > 0
        has_zero_markers = total_markers == 0
        
        # í‚¤ì›Œë“œê°€ ìˆì§€ë§Œ ë§ˆì»¤ê°€ 0ì¸ ê²½ìš°ë§Œ ê·¸ë¦°ì›Œì‹±ìœ¼ë¡œ ì˜ì‹¬
        is_inconsistent = has_keywords and has_zero_markers
        
        # ë¶ˆì¼ì¹˜ ì ìˆ˜ ê³„ì‚° (ML ì¤‘ì‹¬)
        inconsistency_score = 0
        risk_level = 'MINIMAL'
        
        if is_inconsistent:
            # ML í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° (0-100)
            inconsistency_score = int(ml_probability * 100)
            
            # í‚¤ì›Œë“œ ê°œìˆ˜ì— ë”°ë¥¸ ë³´ì •
            keyword_bonus = min(len(detected_keywords) * 5, 20)  # ìµœëŒ€ 20ì  ì¶”ê°€
            inconsistency_score += keyword_bonus
        
        inconsistency_score = min(100, inconsistency_score)
        
        # ìœ„í—˜ë„ ë ˆë²¨ (ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        if inconsistency_score >= 70:
            risk_level = 'CRITICAL'
        elif inconsistency_score >= 50:
            risk_level = 'HIGH'
        elif inconsistency_score >= 30:
            risk_level = 'MEDIUM'
        elif inconsistency_score >= 10:
            risk_level = 'LOW'
        else:
            risk_level = 'MINIMAL'
        
        return {
            'title': title,
            'detected_keywords': detected_keywords,
            'total_markers': total_markers,
            'is_inconsistent': is_inconsistent,
            'inconsistency_score': inconsistency_score,
            'risk_level': risk_level,
            'keyword_count': len(detected_keywords),
            'keyword_density': len(detected_keywords) / max(len(title_clean.split()), 1) if title_clean else 0,
            'ml_probability': ml_probability,
            'has_ml_model': self.ml_model is not None
        }
    
    def batch_analyze(self, projects_df):
        """
        ì—¬ëŸ¬ í”„ë¡œì íŠ¸ ì¼ê´„ ë¶„ì„
        """
        results = []
        
        for idx, row in projects_df.iterrows():
            project_data = {
                'ProjectTitle': row['ProjectTitle'],
                'ClimateMitigation': row.get('ClimateMitigation', 0),
                'ClimateAdaptation': row.get('ClimateAdaptation', 0),
                'Environment': row.get('Environment', 0),
                'Biodiversity': row.get('Biodiversity', 0)
            }
            
            result = self.detect_greenwashing(project_data)
            result['project_index'] = idx
            results.append(result)
        
        # ìš”ì•½ í†µê³„
        total_projects = len(results)
        inconsistent_projects = len([r for r in results if r['is_inconsistent']])
        high_risk_projects = len([r for r in results if r['risk_level'] in ['CRITICAL', 'HIGH']])
        
        summary = {
            'total_projects': total_projects,
            'inconsistent_projects': inconsistent_projects,
            'inconsistency_rate': inconsistent_projects / total_projects * 100 if total_projects > 0 else 0,
            'high_risk_projects': high_risk_projects,
            'high_risk_rate': high_risk_projects / total_projects * 100 if total_projects > 0 else 0
        }
        
        return {
            'project_results': results,
            'summary': summary
        }

# ë„êµ¬ ì´ˆê¸°í™”
detector = IndependentGreenwashingDetector()

# ìƒ˜í”Œ í”„ë¡œì íŠ¸ ë°ì´í„° ìƒì„± (ë” ë‹¤ì–‘í•œ ì¼€ì´ìŠ¤)
sample_projects = pd.DataFrame([
    {
        'ProjectTitle': 'Green Energy Development Project',
        'ClimateMitigation': 2,
        'ClimateAdaptation': 1,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'Germany',
        'USD_Commitment': 2000000
    },
    {
        'ProjectTitle': 'Sustainable Climate Initiative',  # ê·¸ë¦°ì›Œì‹± ì˜ì‹¬: í‚¤ì›Œë“œ O, ë§ˆì»¤ X
        'ClimateMitigation': 0,
        'ClimateAdaptation': 0,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'Korea',
        'USD_Commitment': 1500000
    },
    {
        'ProjectTitle': 'Solar Power Infrastructure',  # ì •ìƒ: í‚¤ì›Œë“œ O, ë§ˆì»¤ O
        'ClimateMitigation': 2,
        'ClimateAdaptation': 0,
        'Environment': 1,
        'Biodiversity': 0,
        'DonorName': 'Japan',
        'USD_Commitment': 3000000
    },
    {
        'ProjectTitle': 'Green Forest Conservation Program',  # ê·¸ë¦°ì›Œì‹± ì˜ì‹¬: í‚¤ì›Œë“œ O, ë§ˆì»¤ X
        'ClimateMitigation': 0,
        'ClimateAdaptation': 0,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'USA',
        'USD_Commitment': 800000
    },
    {
        'ProjectTitle': 'Road Construction Project',  # ì •ìƒ: í‚¤ì›Œë“œ X, ë§ˆì»¤ X
        'ClimateMitigation': 0,
        'ClimateAdaptation': 0,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'France',
        'USD_Commitment': 5000000
    },
    {
        'ProjectTitle': 'Green Sustainable Clean Energy Climate Initiative',  # ê³ ìœ„í—˜: í‚¤ì›Œë“œ ë§ìŒ, ë§ˆì»¤ X
        'ClimateMitigation': 0,
        'ClimateAdaptation': 0,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'Netherlands',
        'USD_Commitment': 2500000
    },
    {
        'ProjectTitle': 'Education Infrastructure Development',  # ì •ìƒ: í‚¤ì›Œë“œ X, ë§ˆì»¤ X
        'ClimateMitigation': 0,
        'ClimateAdaptation': 0,
        'Environment': 0,
        'Biodiversity': 0,
        'DonorName': 'UK',
        'USD_Commitment': 1800000
    },
    {
        'ProjectTitle': 'Renewable Wind Farm Project',  # ì •ìƒ: í‚¤ì›Œë“œ O, ë§ˆì»¤ O
        'ClimateMitigation': 2,
        'ClimateAdaptation': 0,
        'Environment': 2,
        'Biodiversity': 1,
        'DonorName': 'Denmark',
        'USD_Commitment': 4000000
    }
])

print("=== ìƒ˜í”Œ í”„ë¡œì íŠ¸ ë°ì´í„° ===")
print(f"ì´ {len(sample_projects)}ê°œ í”„ë¡œì íŠ¸")
for idx, row in sample_projects.iterrows():
    print(f"{idx+1}. {row['ProjectTitle']}")

print("\n=== ëŒ€ëŸ‰ ë¶„ì„ ì‹¤í–‰ ì¤‘... ===")

# 5. ëŒ€ëŸ‰ í”„ë¡œì íŠ¸ ì¼ê´„ ë¶„ì„
batch_result = detector.batch_analyze(sample_projects)

print("\n=== ëŒ€ëŸ‰ ë¶„ì„ ê²°ê³¼ ===")
print(f"ì´ í”„ë¡œì íŠ¸: {batch_result['summary']['total_projects']}ê°œ")
print(f"ë¶ˆì¼ì¹˜ í”„ë¡œì íŠ¸: {batch_result['summary']['inconsistent_projects']}ê°œ")
print(f"ë¶ˆì¼ì¹˜ìœ¨: {batch_result['summary']['inconsistency_rate']:.1f}%")
print(f"ê³ ìœ„í—˜ í”„ë¡œì íŠ¸: {batch_result['summary']['high_risk_projects']}ê°œ")
print(f"ê³ ìœ„í—˜ë¥ : {batch_result['summary']['high_risk_rate']:.1f}%")

print("\n=== ê°œë³„ í”„ë¡œì íŠ¸ ìƒì„¸ ê²°ê³¼ ===")
for i, result in enumerate(batch_result['project_results']):
    status_icon = "âš ï¸" if result['is_inconsistent'] else "âœ…"
    print(f"\n{status_icon} [í”„ë¡œì íŠ¸ {i+1}] {result['title']}")
    print(f"  - íƒì§€ëœ í‚¤ì›Œë“œ: {result['detected_keywords']} ({result['keyword_count']}ê°œ)")
    print(f"  - í‚¤ì›Œë“œ ë°€ë„: {result['keyword_density']:.2%}")
    print(f"  - í™˜ê²½ ë§ˆì»¤ ì´í•©: {result['total_markers']}")
    print(f"  - ê·¸ë¦°ì›Œì‹± ì˜ì‹¬: {'ì˜ˆ' if result['is_inconsistent'] else 'ì•„ë‹ˆì˜¤'}")
    print(f"  - ë¶ˆì¼ì¹˜ ì ìˆ˜: {result['inconsistency_score']}/100")
    print(f"  - ìœ„í—˜ë„: {result['risk_level']}")
    print(f"  - ML ì˜ˆì¸¡ í™•ë¥ : {result['ml_probability']:.3f}")

print("\n=== ë¶„ì„ ìš”ì•½ ===")
print("ğŸ” ë¶„ë¥˜ ê²°ê³¼:")
risk_counts = {}
for result in batch_result['project_results']:
    risk_level = result['risk_level']
    risk_counts[risk_level] = risk_counts.get(risk_level, 0) + 1

for risk_level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW', 'MINIMAL']:
    count = risk_counts.get(risk_level, 0)
    if count > 0:
        print(f"  - {risk_level}: {count}ê°œ")

print("\n=== ê·¸ë¦°ì›Œì‹± ì˜ì‹¬ í”„ë¡œì íŠ¸ ===")
suspicious_projects = [r for r in batch_result['project_results'] if r['is_inconsistent']]
if suspicious_projects:
    print(f"ì´ {len(suspicious_projects)}ê°œ í”„ë¡œì íŠ¸ê°€ ê·¸ë¦°ì›Œì‹±ìœ¼ë¡œ ì˜ì‹¬ë©ë‹ˆë‹¤:")
    for result in suspicious_projects:
        print(f"âš ï¸  {result['title']}")
        print(f"    í‚¤ì›Œë“œ: {result['detected_keywords']}")
        print(f"    ìœ„í—˜ë„: {result['risk_level']} ({result['inconsistency_score']}/100)")
        print(f"    ML í™•ë¥ : {result['ml_probability']:.3f}")
        print(f"    ë¬¸ì œ: í™˜ê²½ í‚¤ì›Œë“œëŠ” ìˆì§€ë§Œ í™˜ê²½ ë§ˆì»¤ê°€ 0")
else:
    print("âœ… ê·¸ë¦°ì›Œì‹± ì˜ì‹¬ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

